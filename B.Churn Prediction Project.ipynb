{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fade3f3-a285-4969-bd2b-59d8bc73f708",
   "metadata": {},
   "source": [
    "# Internshala Project Churn Prediction -part B"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "519a2eab-ffbf-4f85-b2b0-ecdff1097d7a",
   "metadata": {},
   "source": [
    "# Video Explanation Link : https://drive.google.com/file/d/1JlA2dc3MUCV0HADQeUOzJ9mzXDNvEbtU/view?usp=sharing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "807b02fb-447a-41b2-82dd-b6f19f6dfd8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7b991527-c36e-4c71-a787-652e67b34137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7043, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerID  gender  SeniorCitizen Partner Dependents  tenure PhoneService  \\\n",
       "0  7590-VHVEG  Female              0     Yes         No       1           No   \n",
       "1  5575-GNVDE    Male              0      No         No      34          Yes   \n",
       "2  3668-QPYBK    Male              0      No         No       2          Yes   \n",
       "3  7795-CFOCW    Male              0      No         No      45           No   \n",
       "4  9237-HQITU  Female              0      No         No       2          Yes   \n",
       "\n",
       "      MultipleLines InternetService OnlineSecurity  ... DeviceProtection  \\\n",
       "0  No phone service             DSL             No  ...               No   \n",
       "1                No             DSL            Yes  ...              Yes   \n",
       "2                No             DSL            Yes  ...               No   \n",
       "3  No phone service             DSL            Yes  ...              Yes   \n",
       "4                No     Fiber optic             No  ...               No   \n",
       "\n",
       "  TechSupport StreamingTV StreamingMovies        Contract PaperlessBilling  \\\n",
       "0          No          No              No  Month-to-month              Yes   \n",
       "1          No          No              No        One year               No   \n",
       "2          No          No              No  Month-to-month              Yes   \n",
       "3         Yes          No              No        One year               No   \n",
       "4          No          No              No  Month-to-month              Yes   \n",
       "\n",
       "               PaymentMethod MonthlyCharges  TotalCharges  Churn  \n",
       "0           Electronic check          29.85         29.85     No  \n",
       "1               Mailed check          56.95       1889.50     No  \n",
       "2               Mailed check          53.85        108.15    Yes  \n",
       "3  Bank transfer (automatic)          42.30       1840.75     No  \n",
       "4           Electronic check          70.70        151.65    Yes  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(\"Customer_data.xlsx\")\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3f8d5d5-6cdb-4590-9dac-c8529f1e6cbc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>tenure</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>7043.000000</td>\n",
       "      <td>7043.000000</td>\n",
       "      <td>7043.000000</td>\n",
       "      <td>7032.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.162147</td>\n",
       "      <td>32.371149</td>\n",
       "      <td>64.761692</td>\n",
       "      <td>2283.300441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.368612</td>\n",
       "      <td>24.559481</td>\n",
       "      <td>30.090047</td>\n",
       "      <td>2266.771362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>18.250000</td>\n",
       "      <td>18.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.000000</td>\n",
       "      <td>35.500000</td>\n",
       "      <td>401.450000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>70.350000</td>\n",
       "      <td>1397.475000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>89.850000</td>\n",
       "      <td>3794.737500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>72.000000</td>\n",
       "      <td>118.750000</td>\n",
       "      <td>8684.800000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       SeniorCitizen       tenure  MonthlyCharges  TotalCharges\n",
       "count    7043.000000  7043.000000     7043.000000   7032.000000\n",
       "mean        0.162147    32.371149       64.761692   2283.300441\n",
       "std         0.368612    24.559481       30.090047   2266.771362\n",
       "min         0.000000     0.000000       18.250000     18.800000\n",
       "25%         0.000000     9.000000       35.500000    401.450000\n",
       "50%         0.000000    29.000000       70.350000   1397.475000\n",
       "75%         0.000000    55.000000       89.850000   3794.737500\n",
       "max         1.000000    72.000000      118.750000   8684.800000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "89550bb4-43e6-4217-9653-b8368c0d8ef7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7043 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7043 non-null   object \n",
      " 1   gender            7043 non-null   object \n",
      " 2   SeniorCitizen     7043 non-null   int64  \n",
      " 3   Partner           7043 non-null   object \n",
      " 4   Dependents        7043 non-null   object \n",
      " 5   tenure            7043 non-null   int64  \n",
      " 6   PhoneService      7043 non-null   object \n",
      " 7   MultipleLines     7043 non-null   object \n",
      " 8   InternetService   7043 non-null   object \n",
      " 9   OnlineSecurity    7043 non-null   object \n",
      " 10  OnlineBackup      7043 non-null   object \n",
      " 11  DeviceProtection  7043 non-null   object \n",
      " 12  TechSupport       7043 non-null   object \n",
      " 13  StreamingTV       7043 non-null   object \n",
      " 14  StreamingMovies   7043 non-null   object \n",
      " 15  Contract          7043 non-null   object \n",
      " 16  PaperlessBilling  7043 non-null   object \n",
      " 17  PaymentMethod     7043 non-null   object \n",
      " 18  MonthlyCharges    7043 non-null   float64\n",
      " 19  TotalCharges      7032 non-null   float64\n",
      " 20  Churn             7043 non-null   object \n",
      "dtypes: float64(2), int64(2), object(17)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59897f98-4e58-4d17-96bc-24ef7ede373b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerID</th>\n",
       "      <th>gender</th>\n",
       "      <th>SeniorCitizen</th>\n",
       "      <th>Partner</th>\n",
       "      <th>Dependents</th>\n",
       "      <th>tenure</th>\n",
       "      <th>PhoneService</th>\n",
       "      <th>MultipleLines</th>\n",
       "      <th>InternetService</th>\n",
       "      <th>OnlineSecurity</th>\n",
       "      <th>...</th>\n",
       "      <th>DeviceProtection</th>\n",
       "      <th>TechSupport</th>\n",
       "      <th>StreamingTV</th>\n",
       "      <th>StreamingMovies</th>\n",
       "      <th>Contract</th>\n",
       "      <th>PaperlessBilling</th>\n",
       "      <th>PaymentMethod</th>\n",
       "      <th>MonthlyCharges</th>\n",
       "      <th>TotalCharges</th>\n",
       "      <th>Churn</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7590-VHVEG</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>1</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.85</td>\n",
       "      <td>29.85</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5575-GNVDE</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>34</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>56.95</td>\n",
       "      <td>1889.50</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3668-QPYBK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>53.85</td>\n",
       "      <td>108.15</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7795-CFOCW</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>45</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>One year</td>\n",
       "      <td>No</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>42.30</td>\n",
       "      <td>1840.75</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9237-HQITU</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>2</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>70.70</td>\n",
       "      <td>151.65</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7038</th>\n",
       "      <td>6840-RESVB</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>24</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>One year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>84.80</td>\n",
       "      <td>1990.50</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7039</th>\n",
       "      <td>2234-XADUH</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>72</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>One year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Credit card (automatic)</td>\n",
       "      <td>103.20</td>\n",
       "      <td>7362.90</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7040</th>\n",
       "      <td>4801-JZAZL</td>\n",
       "      <td>Female</td>\n",
       "      <td>0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>11</td>\n",
       "      <td>No</td>\n",
       "      <td>No phone service</td>\n",
       "      <td>DSL</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Electronic check</td>\n",
       "      <td>29.60</td>\n",
       "      <td>346.45</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7041</th>\n",
       "      <td>8361-LTMKD</td>\n",
       "      <td>Male</td>\n",
       "      <td>1</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>4</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>No</td>\n",
       "      <td>...</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>Month-to-month</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Mailed check</td>\n",
       "      <td>74.40</td>\n",
       "      <td>306.60</td>\n",
       "      <td>Yes</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7042</th>\n",
       "      <td>3186-AJIEK</td>\n",
       "      <td>Male</td>\n",
       "      <td>0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>66</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>Fiber optic</td>\n",
       "      <td>Yes</td>\n",
       "      <td>...</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Two year</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Bank transfer (automatic)</td>\n",
       "      <td>105.65</td>\n",
       "      <td>6844.50</td>\n",
       "      <td>No</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>7032 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      customerID  gender  SeniorCitizen Partner Dependents  tenure  \\\n",
       "0     7590-VHVEG  Female              0     Yes         No       1   \n",
       "1     5575-GNVDE    Male              0      No         No      34   \n",
       "2     3668-QPYBK    Male              0      No         No       2   \n",
       "3     7795-CFOCW    Male              0      No         No      45   \n",
       "4     9237-HQITU  Female              0      No         No       2   \n",
       "...          ...     ...            ...     ...        ...     ...   \n",
       "7038  6840-RESVB    Male              0     Yes        Yes      24   \n",
       "7039  2234-XADUH  Female              0     Yes        Yes      72   \n",
       "7040  4801-JZAZL  Female              0     Yes        Yes      11   \n",
       "7041  8361-LTMKD    Male              1     Yes         No       4   \n",
       "7042  3186-AJIEK    Male              0      No         No      66   \n",
       "\n",
       "     PhoneService     MultipleLines InternetService OnlineSecurity  ...  \\\n",
       "0              No  No phone service             DSL             No  ...   \n",
       "1             Yes                No             DSL            Yes  ...   \n",
       "2             Yes                No             DSL            Yes  ...   \n",
       "3              No  No phone service             DSL            Yes  ...   \n",
       "4             Yes                No     Fiber optic             No  ...   \n",
       "...           ...               ...             ...            ...  ...   \n",
       "7038          Yes               Yes             DSL            Yes  ...   \n",
       "7039          Yes               Yes     Fiber optic             No  ...   \n",
       "7040           No  No phone service             DSL            Yes  ...   \n",
       "7041          Yes               Yes     Fiber optic             No  ...   \n",
       "7042          Yes                No     Fiber optic            Yes  ...   \n",
       "\n",
       "     DeviceProtection TechSupport StreamingTV StreamingMovies        Contract  \\\n",
       "0                  No          No          No              No  Month-to-month   \n",
       "1                 Yes          No          No              No        One year   \n",
       "2                  No          No          No              No  Month-to-month   \n",
       "3                 Yes         Yes          No              No        One year   \n",
       "4                  No          No          No              No  Month-to-month   \n",
       "...               ...         ...         ...             ...             ...   \n",
       "7038              Yes         Yes         Yes             Yes        One year   \n",
       "7039              Yes          No         Yes             Yes        One year   \n",
       "7040               No          No          No              No  Month-to-month   \n",
       "7041               No          No          No              No  Month-to-month   \n",
       "7042              Yes         Yes         Yes             Yes        Two year   \n",
       "\n",
       "     PaperlessBilling              PaymentMethod MonthlyCharges  TotalCharges  \\\n",
       "0                 Yes           Electronic check          29.85         29.85   \n",
       "1                  No               Mailed check          56.95       1889.50   \n",
       "2                 Yes               Mailed check          53.85        108.15   \n",
       "3                  No  Bank transfer (automatic)          42.30       1840.75   \n",
       "4                 Yes           Electronic check          70.70        151.65   \n",
       "...               ...                        ...            ...           ...   \n",
       "7038              Yes               Mailed check          84.80       1990.50   \n",
       "7039              Yes    Credit card (automatic)         103.20       7362.90   \n",
       "7040              Yes           Electronic check          29.60        346.45   \n",
       "7041              Yes               Mailed check          74.40        306.60   \n",
       "7042              Yes  Bank transfer (automatic)         105.65       6844.50   \n",
       "\n",
       "      Churn  \n",
       "0        No  \n",
       "1        No  \n",
       "2       Yes  \n",
       "3        No  \n",
       "4       Yes  \n",
       "...     ...  \n",
       "7038     No  \n",
       "7039     No  \n",
       "7040     No  \n",
       "7041    Yes  \n",
       "7042     No  \n",
       "\n",
       "[7032 rows x 21 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de0b95ca-0232-412a-b485-2b09d6d980a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')\n",
    "df.dropna(inplace=True)  # Drop rows with missing TotalCharges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9d976480-b2a1-43fa-9921-fc49b7be74fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 7032 entries, 0 to 7042\n",
      "Data columns (total 21 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   customerID        7032 non-null   object \n",
      " 1   gender            7032 non-null   object \n",
      " 2   SeniorCitizen     7032 non-null   int64  \n",
      " 3   Partner           7032 non-null   object \n",
      " 4   Dependents        7032 non-null   object \n",
      " 5   tenure            7032 non-null   int64  \n",
      " 6   PhoneService      7032 non-null   object \n",
      " 7   MultipleLines     7032 non-null   object \n",
      " 8   InternetService   7032 non-null   object \n",
      " 9   OnlineSecurity    7032 non-null   object \n",
      " 10  OnlineBackup      7032 non-null   object \n",
      " 11  DeviceProtection  7032 non-null   object \n",
      " 12  TechSupport       7032 non-null   object \n",
      " 13  StreamingTV       7032 non-null   object \n",
      " 14  StreamingMovies   7032 non-null   object \n",
      " 15  Contract          7032 non-null   object \n",
      " 16  PaperlessBilling  7032 non-null   object \n",
      " 17  PaymentMethod     7032 non-null   object \n",
      " 18  MonthlyCharges    7032 non-null   float64\n",
      " 19  TotalCharges      7032 non-null   float64\n",
      " 20  Churn             7032 non-null   object \n",
      "dtypes: float64(2), int64(2), object(17)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "66df205e-1c81-47fa-81a6-02d4fcd7ccd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "\n",
    "categorical_cols = df.select_dtypes(include = 'object').columns.tolist()\n",
    "\n",
    "#Removing CustomerID from encoding\n",
    "categorical_cols.remove('customerID')\n",
    "\n",
    "for col in categorical_cols:\n",
    "    df[col] = le.fit_transform(df[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c3d015d1-d2b1-4beb-b934-0dd7dce18ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns = ['customerID'], inplace = True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "372da151-a042-4aae-81c9-2fa211ebd55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('Churn', axis = 1)\n",
    "y = df ['Churn']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c1a3bcd7-cdae-4d1c-a839-b0e38911de97",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test = train_test_split(X,y, test_size = 0.2, random_state = 42, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bb2a750e-e1e8-4f3e-9451-1a16d8e94333",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "scaler.fit(X_train)              # Don't convert to array here\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "61a6accf-3636-49e0-9e60-7e5dc6c34175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets tune each and every model and compare them all together\n",
    "#1. Logistic Regression\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score , precision_score , recall_score, f1_score, classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "9304a6f0-4ae1-42b0-aab3-48581216709a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 5 candidates, totalling 25 fits\n"
     ]
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'C' : [0.01,0.1,1,10,100],\n",
    "    'solver' : ['liblinear'],\n",
    "}\n",
    "\n",
    "log_reg = LogisticRegression(class_weight = 'balanced', max_iter = 1000, random_state = 42)\n",
    "grid = GridSearchCV(log_reg, param_grid, cv = 5 , scoring = 'f1', n_jobs = -1, verbose = 1)\n",
    "grid.fit(X_train_scaled,y_train)\n",
    "\n",
    "best_logreg = grid.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "30073060-cfa2-4235-a34c-5b1c1a91040f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Evaluation\n",
      "Best Parameters: {'C': 10, 'solver': 'liblinear'}\n",
      "Accuracy:  0.7264\n",
      "Precision: 0.4909\n",
      "Recall:    0.7968\n",
      "F1 Score:  0.6075\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79      1033\n",
      "           1       0.49      0.80      0.61       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.75      0.70      1407\n",
      "weighted avg       0.79      0.73      0.74      1407\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[724 309]\n",
      " [ 76 298]]\n"
     ]
    }
   ],
   "source": [
    "y_pred = best_logreg.predict(X_test_scaled)\n",
    "\n",
    "print(\"Logistic Regression Evaluation\")\n",
    "print(\"Best Parameters:\", grid.best_params_)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "c743f4e5-a417-450b-abe6-64f8b85aa1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,200],\n",
    "    'max_depth' : [5,10,15],\n",
    "    'min_samples_split' : [2,5],\n",
    "    'min_samples_leaf' : [1,2],\n",
    "    'class_weight' : ['balanced']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "bdc9da36-791c-45ee-8072-d1181b7411c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "Random Forest Evaluation\n",
      "Best Parameters: {'class_weight': 'balanced', 'max_depth': 5, 'min_samples_leaf': 1, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Accuracy:  0.7278\n",
      "Precision: 0.4926\n",
      "Recall:    0.8048\n",
      "F1 Score:  0.6112\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.70      0.79      1033\n",
      "           1       0.49      0.80      0.61       374\n",
      "\n",
      "    accuracy                           0.73      1407\n",
      "   macro avg       0.70      0.75      0.70      1407\n",
      "weighted avg       0.80      0.73      0.74      1407\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[723 310]\n",
      " [ 73 301]]\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(random_state = 42)\n",
    "\n",
    "grid_rf = GridSearchCV(rf, param_grid, cv = 5 , scoring = 'f1' , n_jobs = -1 , verbose = 1)\n",
    "grid_rf.fit (X_train_scaled, y_train)\n",
    "\n",
    "best_rf = grid_rf.best_estimator_\n",
    "\n",
    "y_pred_rf = best_rf.predict(X_test_scaled)\n",
    "print(\"Random Forest Evaluation\")\n",
    "print(\"Best Parameters:\", grid_rf.best_params_)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_rf):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_rf):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_rf))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9e854854-8080-466c-8b73-bc7ef05c404a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: xgboost in c:\\users\\danis\\anaconda3\\lib\\site-packages (3.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\danis\\anaconda3\\lib\\site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\danis\\anaconda3\\lib\\site-packages (from xgboost) (1.13.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost\n",
    "from xgboost import XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "66fe7bc1-b2b9-4056-92b6-da3de773e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "param_grid = {\n",
    "    'n_estimators' : [100,200],\n",
    "    'max_depth' : [3,5,7],\n",
    "    'learning_rate' : [0.01,0.1],\n",
    "    'subsample' : [0.8,1],\n",
    "    'colsample_bytree' : [0.8,1],\n",
    "    'scale_pos_weight' : [1,3]   \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "3bb728cb-95f9-4749-be5c-e10c1bbf25ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 96 candidates, totalling 480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\anaconda3\\Lib\\site-packages\\xgboost\\training.py:183: UserWarning: [23:56:27] WARNING: C:\\actions-runner\\_work\\xgboost\\xgboost\\src\\learner.cc:738: \n",
      "Parameters: { \"eval_metrics\", \"use_label_encoder\" } are not used.\n",
      "\n",
      "  bst.update(dtrain, iteration=i, fobj=obj)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost Classifier Evaluation\n",
      "Best Parameters: {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 7, 'n_estimators': 200, 'scale_pos_weight': 3, 'subsample': 0.8}\n",
      "Accuracy:  0.7370\n",
      "Precision: 0.5035\n",
      "Recall:    0.7727\n",
      "F1 Score:  0.6097\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.72      0.80      1033\n",
      "           1       0.50      0.77      0.61       374\n",
      "\n",
      "    accuracy                           0.74      1407\n",
      "   macro avg       0.70      0.75      0.71      1407\n",
      "weighted avg       0.79      0.74      0.75      1407\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[748 285]\n",
      " [ 85 289]]\n"
     ]
    }
   ],
   "source": [
    "xgb = XGBClassifier (random_state = 42 , use_label_encoder = False, eval_metrics = 'logloss')\n",
    "\n",
    "grid_xgb = GridSearchCV ( estimator = xgb, param_grid = param_grid, cv = 5,scoring = 'f1',n_jobs = -1, verbose = 1)\n",
    "\n",
    "grid_xgb.fit(X_train_scaled , y_train)\n",
    "best_xgb = grid_xgb.best_estimator_\n",
    "\n",
    "y_pred_xgb = best_xgb.predict(X_test_scaled)\n",
    "print(\"XGBoost Classifier Evaluation\")\n",
    "print(\"Best Parameters:\", grid_xgb.best_params_)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_xgb):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_xgb))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_xgb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f42762ce-131b-4dc9-a61f-0e47c0320dee",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class CatBoostClassifier in module catboost.core:\n",
      "\n",
      "class CatBoostClassifier(CatBoost)\n",
      " |  CatBoostClassifier(iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |\n",
      " |  Implementation of the scikit-learn API for CatBoost classification.\n",
      " |\n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  iterations : int, [default=500]\n",
      " |      Max count of trees.\n",
      " |      range: [1,+inf)\n",
      " |  learning_rate : float, [default value is selected automatically for binary classification with other parameters set to default. In all other cases default is 0.03]\n",
      " |      Step size shrinkage used in update to prevents overfitting.\n",
      " |      range: (0,1]\n",
      " |  depth : int, [default=6]\n",
      " |      Depth of a tree. All trees are the same depth.\n",
      " |      range: [1,16]\n",
      " |  l2_leaf_reg : float, [default=3.0]\n",
      " |      Coefficient at the L2 regularization term of the cost function.\n",
      " |      range: [0,+inf)\n",
      " |  model_size_reg : float, [default=None]\n",
      " |      Model size regularization coefficient.\n",
      " |      range: [0,+inf)\n",
      " |  rsm : float, [default=None]\n",
      " |      Subsample ratio of columns when constructing each tree.\n",
      " |      range: (0,1]\n",
      " |  loss_function : string or object, [default='Logloss']\n",
      " |      The metric to use in training and also selector of the machine learning\n",
      " |      problem to solve. If string, then the name of a supported metric,\n",
      " |      optionally suffixed with parameter description.\n",
      " |      If object, it shall provide methods 'calc_ders_range' or 'calc_ders_multi'.\n",
      " |  border_count : int, [default = 254 for training on CPU or 128 for training on GPU]\n",
      " |      The number of partitions in numeric features binarization. Used in the preliminary calculation.\n",
      " |      range: [1,65535] on CPU, [1,255] on GPU\n",
      " |  feature_border_type : string, [default='GreedyLogSum']\n",
      " |      The binarization mode in numeric features binarization. Used in the preliminary calculation.\n",
      " |      Possible values:\n",
      " |          - 'Median'\n",
      " |          - 'Uniform'\n",
      " |          - 'UniformAndQuantiles'\n",
      " |          - 'GreedyLogSum'\n",
      " |          - 'MaxLogSum'\n",
      " |          - 'MinEntropy'\n",
      " |  per_float_feature_quantization : list of strings, [default=None]\n",
      " |      List of float binarization descriptions.\n",
      " |      Format : described in documentation on catboost.ai\n",
      " |      Example 1: ['0:1024'] means that feature 0 will have 1024 borders.\n",
      " |      Example 2: ['0:border_count=1024', '1:border_count=1024', ...] means that two first features have 1024 borders.\n",
      " |      Example 3: ['0:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum',\n",
      " |                  '1:nan_mode=Forbidden,border_count=32,border_type=GreedyLogSum'] - defines more quantization properties for first two features.\n",
      " |  input_borders : string or pathlib.Path, [default=None]\n",
      " |      input file with borders used in numeric features binarization.\n",
      " |  output_borders : string, [default=None]\n",
      " |      output file for borders that were used in numeric features binarization.\n",
      " |  fold_permutation_block : int, [default=1]\n",
      " |      To accelerate the learning.\n",
      " |      The recommended value is within [1, 256]. On small samples, must be set to 1.\n",
      " |      range: [1,+inf)\n",
      " |  od_pval : float, [default=None]\n",
      " |      Use overfitting detector to stop training when reaching a specified threshold.\n",
      " |      Can be used only with eval_set.\n",
      " |      range: [0,1]\n",
      " |  od_wait : int, [default=None]\n",
      " |      Number of iterations which overfitting detector will wait after new best error.\n",
      " |  od_type : string, [default=None]\n",
      " |      Type of overfitting detector which will be used in program.\n",
      " |      Posible values:\n",
      " |          - 'IncToDec'\n",
      " |          - 'Iter'\n",
      " |      For 'Iter' type od_pval must not be set.\n",
      " |      If None, then od_type=IncToDec.\n",
      " |  nan_mode : string, [default=None]\n",
      " |      Way to process missing values for numeric features.\n",
      " |      Possible values:\n",
      " |          - 'Forbidden' - raises an exception if there is a missing value for a numeric feature in a dataset.\n",
      " |          - 'Min' - each missing value will be processed as the minimum numerical value.\n",
      " |          - 'Max' - each missing value will be processed as the maximum numerical value.\n",
      " |      If None, then nan_mode=Min.\n",
      " |  counter_calc_method : string, [default=None]\n",
      " |      The method used to calculate counters for dataset with Counter type.\n",
      " |      Possible values:\n",
      " |          - 'PrefixTest' - only objects up to current in the test dataset are considered\n",
      " |          - 'FullTest' - all objects are considered in the test dataset\n",
      " |          - 'SkipTest' - Objects from test dataset are not considered\n",
      " |          - 'Full' - all objects are considered for both learn and test dataset\n",
      " |      If None, then counter_calc_method=PrefixTest.\n",
      " |  leaf_estimation_iterations : int, [default=None]\n",
      " |      The number of steps in the gradient when calculating the values in the leaves.\n",
      " |      If None, then leaf_estimation_iterations=1.\n",
      " |      range: [1,+inf)\n",
      " |  leaf_estimation_method : string, [default=None]\n",
      " |      The method used to calculate the values in the leaves.\n",
      " |      Possible values:\n",
      " |          - 'Newton'\n",
      " |          - 'Gradient'\n",
      " |  thread_count : int, [default=None]\n",
      " |      Number of parallel threads used to run CatBoost.\n",
      " |      If None or -1, then the number of threads is set to the number of CPU cores.\n",
      " |      range: [1,+inf)\n",
      " |  random_seed : int, [default=None]\n",
      " |      Random number seed.\n",
      " |      If None, 0 is used.\n",
      " |      range: [0,+inf)\n",
      " |  use_best_model : bool, [default=None]\n",
      " |      To limit the number of trees in predict() using information about the optimal value of the error function.\n",
      " |      Can be used only with eval_set.\n",
      " |  best_model_min_trees : int, [default=None]\n",
      " |      The minimal number of trees the best model should have.\n",
      " |  verbose: bool\n",
      " |      When set to True, logging_level is set to 'Verbose'.\n",
      " |      When set to False, logging_level is set to 'Silent'.\n",
      " |  silent: bool, synonym for verbose\n",
      " |  logging_level : string, [default='Verbose']\n",
      " |      Possible values:\n",
      " |          - 'Silent'\n",
      " |          - 'Verbose'\n",
      " |          - 'Info'\n",
      " |          - 'Debug'\n",
      " |  metric_period : int, [default=1]\n",
      " |      The frequency of iterations to print the information to stdout. The value should be a positive integer.\n",
      " |  simple_ctr: list of strings, [default=None]\n",
      " |      Binarization settings for categorical features.\n",
      " |          Format : see documentation\n",
      " |          Example: ['Borders:CtrBorderCount=5:Prior=0:Prior=0.5', 'BinarizedTargetMeanValue:TargetBorderCount=10:TargetBorderType=MinEntropy', ...]\n",
      " |          CTR types:\n",
      " |              CPU and GPU\n",
      " |              - 'Borders'\n",
      " |              - 'Buckets'\n",
      " |              CPU only\n",
      " |              - 'BinarizedTargetMeanValue'\n",
      " |              - 'Counter'\n",
      " |              GPU only\n",
      " |              - 'FloatTargetMeanValue'\n",
      " |              - 'FeatureFreq'\n",
      " |          Number_of_borders, binarization type, target borders and binarizations, priors are optional parametrs\n",
      " |  combinations_ctr: list of strings, [default=None]\n",
      " |  per_feature_ctr: list of strings, [default=None]\n",
      " |  ctr_target_border_count: int, [default=None]\n",
      " |      Maximum number of borders used in target binarization for categorical features that need it.\n",
      " |      If TargetBorderCount is specified in 'simple_ctr', 'combinations_ctr' or 'per_feature_ctr' option it\n",
      " |      overrides this value.\n",
      " |      range: [1, 255]\n",
      " |  ctr_leaf_count_limit : int, [default=None]\n",
      " |      The maximum number of leaves with categorical features.\n",
      " |      If the number of leaves exceeds the specified limit, some leaves are discarded.\n",
      " |      The leaves to be discarded are selected as follows:\n",
      " |          - The leaves are sorted by the frequency of the values.\n",
      " |          - The top N leaves are selected, where N is the value specified in the parameter.\n",
      " |          - All leaves starting from N+1 are discarded.\n",
      " |      This option reduces the resulting model size\n",
      " |      and the amount of memory required for training.\n",
      " |      Note that the resulting quality of the model can be affected.\n",
      " |      range: [1,+inf) (for zero limit use ignored_features)\n",
      " |  store_all_simple_ctr : bool, [default=None]\n",
      " |      Ignore categorical features, which are not used in feature combinations,\n",
      " |      when choosing candidates for exclusion.\n",
      " |      Use this parameter with ctr_leaf_count_limit only.\n",
      " |  max_ctr_complexity : int, [default=4]\n",
      " |      The maximum number of Categ features that can be combined.\n",
      " |      range: [0,+inf)\n",
      " |  has_time : bool, [default=False]\n",
      " |      To use the order in which objects are represented in the input data\n",
      " |      (do not perform a random permutation of the dataset at the preprocessing stage).\n",
      " |  allow_const_label : bool, [default=False]\n",
      " |      To allow the constant label value in dataset.\n",
      " |  target_border: float, [default=None]\n",
      " |      Border for target binarization.\n",
      " |  classes_count : int, [default=None]\n",
      " |      The upper limit for the numeric class label.\n",
      " |      Defines the number of classes for multiclassification.\n",
      " |      Only non-negative integers can be specified.\n",
      " |      The given integer should be greater than any of the target values.\n",
      " |      If this parameter is specified the labels for all classes in the input dataset\n",
      " |      should be smaller than the given value.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  class_weights : list or dict, [default=None]\n",
      " |      Classes weights. The values are used as multipliers for the object weights.\n",
      " |      If None, all classes are supposed to have weight one.\n",
      " |      If list - class weights in order of class_names or sequential classes if class_names is undefined\n",
      " |      If dict - dict of class_name -> class_weight.\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  auto_class_weights : string [default=None]\n",
      " |      Enables automatic class weights calculation. Possible values:\n",
      " |          - Balanced  # weight = maxSummaryClassWeight / summaryClassWeight, statistics determined from train pool\n",
      " |          - SqrtBalanced  # weight = sqrt(maxSummaryClassWeight / summaryClassWeight)\n",
      " |  class_names: list of strings, [default=None]\n",
      " |      Class names. Allows to redefine the default values for class labels (integer numbers).\n",
      " |      If several of 'classes_count', 'class_weights', 'class_names' parameters are defined\n",
      " |      the numbers of classes specified by each of them must be equal.\n",
      " |  one_hot_max_size : int, [default=None]\n",
      " |      Convert the feature to float\n",
      " |      if the number of different values that it takes exceeds the specified value.\n",
      " |      Ctrs are not calculated for such features.\n",
      " |  random_strength : float, [default=1]\n",
      " |      Score standard deviation multiplier.\n",
      " |  random_score_type : string [default=None]\n",
      " |      Type of random noise added to scores.\n",
      " |      Possible values:\n",
      " |          - 'Gumbel' - Gumbel-distributed\n",
      " |          - 'NormalWithModelSizeDecrease' - Normally-distributed with deviation decreasing with model iteration count\n",
      " |      If None than 'NormalWithModelSizeDecrease' will be used by default.\n",
      " |  name : string, [default='experiment']\n",
      " |      The name that should be displayed in the visualization tools.\n",
      " |  ignored_features : list, [default=None]\n",
      " |      Indices or names of features that should be excluded when training.\n",
      " |  train_dir : string or pathlib.Path, [default=None]\n",
      " |      The directory in which you want to record generated in the process of learning files.\n",
      " |  custom_metric : string or list of strings, [default=None]\n",
      " |      To use your own metric function.\n",
      " |  custom_loss: alias to custom_metric\n",
      " |  eval_metric : string or object, [default=None]\n",
      " |      To optimize your custom metric in loss.\n",
      " |  bagging_temperature : float, [default=None]\n",
      " |      Controls intensity of Bayesian bagging. The higher the temperature the more aggressive bagging is.\n",
      " |      Typical values are in range [0, 1] (0 - no bagging, 1 - default).\n",
      " |  save_snapshot : bool, [default=None]\n",
      " |      Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |  snapshot_file : string or pathlib.Path, [default=None]\n",
      " |      Learn progress snapshot file path, if None will use default filename\n",
      " |  snapshot_interval: int, [default=600]\n",
      " |      Interval between saving snapshots (seconds)\n",
      " |  fold_len_multiplier : float, [default=None]\n",
      " |      Fold length multiplier. Should be greater than 1\n",
      " |  used_ram_limit : string or number, [default=None]\n",
      " |      Set a limit on memory consumption (value like '1.2gb' or 1.2e9).\n",
      " |      WARNING: Currently this option affects CTR memory usage only.\n",
      " |  gpu_ram_part : float, [default=0.95]\n",
      " |      Fraction of the GPU RAM to use for training, a value from (0, 1].\n",
      " |  pinned_memory_size: int [default=None]\n",
      " |      Size of additional CPU pinned memory used for GPU learning,\n",
      " |      usually is estimated automatically, thus usually should not be set.\n",
      " |  allow_writing_files : bool, [default=True]\n",
      " |      If this flag is set to False, no files with different diagnostic info will be created during training.\n",
      " |      With this flag no snapshotting can be done. Plus visualisation will not\n",
      " |      work, because visualisation uses files that are created and updated during training.\n",
      " |  final_ctr_computation_mode : string, [default='Default']\n",
      " |      Possible values:\n",
      " |          - 'Default' - Compute final ctrs for all pools.\n",
      " |          - 'Skip' - Skip final ctr computation. WARNING: model without ctrs can't be applied.\n",
      " |  approx_on_full_history : bool, [default=False]\n",
      " |      If this flag is set to True, each approximated value is calculated using all the preceeding rows in the fold (slower, more accurate).\n",
      " |      If this flag is set to False, each approximated value is calculated using only the beginning 1/fold_len_multiplier fraction of the fold (faster, slightly less accurate).\n",
      " |  boosting_type : string, default value depends on object count and feature count in train dataset and on learning mode.\n",
      " |      Boosting scheme.\n",
      " |      Possible values:\n",
      " |          - 'Ordered' - Gives better quality, but may slow down the training.\n",
      " |          - 'Plain' - The classic gradient boosting scheme. May result in quality degradation, but does not slow down the training.\n",
      " |  task_type : string, [default=None]\n",
      " |      The calcer type used to train the model.\n",
      " |      Possible values:\n",
      " |          - 'CPU'\n",
      " |          - 'GPU'\n",
      " |  device_config : string, [default=None], deprecated, use devices instead\n",
      " |  devices : list or string, [default=None], GPU devices to use.\n",
      " |      String format is: '0' for 1 device or '0:1:3' for multiple devices or '0-3' for range of devices.\n",
      " |      List format is : [0] for 1 device or [0,1,3] for multiple devices.\n",
      " |\n",
      " |  bootstrap_type : string, Bayesian, Bernoulli, Poisson, MVS.\n",
      " |      Default bootstrap is Bayesian for GPU and MVS for CPU.\n",
      " |      Poisson bootstrap is supported only on GPU.\n",
      " |      MVS bootstrap is supported only on GPU.\n",
      " |\n",
      " |  subsample : float, [default=None]\n",
      " |      Sample rate for bagging. This parameter can be used Poisson or Bernoully bootstrap types.\n",
      " |\n",
      " |  mvs_reg : float, [default is set automatically at each iteration based on gradient distribution]\n",
      " |      Regularization parameter for MVS sampling algorithm\n",
      " |\n",
      " |  monotone_constraints : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Monotone constraints for features.\n",
      " |\n",
      " |  feature_weights : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Coefficient to multiply split gain with specific feature use. Should be non-negative.\n",
      " |\n",
      " |  penalties_coefficient : float, [default=1]\n",
      " |      Common coefficient for all penalties. Should be non-negative.\n",
      " |\n",
      " |  first_feature_use_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties to first use of specific feature in model. Should be non-negative.\n",
      " |\n",
      " |  per_object_feature_penalties : list or numpy.ndarray or string or dict, [default=None]\n",
      " |      Penalties for first use of feature for each object. Should be non-negative.\n",
      " |\n",
      " |  sampling_frequency : string, [default=PerTree]\n",
      " |      Frequency to sample weights and objects when building trees.\n",
      " |      Possible values:\n",
      " |          - 'PerTree' - Before constructing each new tree\n",
      " |          - 'PerTreeLevel' - Before choosing each new split of a tree\n",
      " |\n",
      " |  sampling_unit : string, [default='Object'].\n",
      " |      Possible values:\n",
      " |          - 'Object'\n",
      " |          - 'Group'\n",
      " |      The parameter allows to specify the sampling scheme:\n",
      " |      sample weights for each object individually or for an entire group of objects together.\n",
      " |\n",
      " |  dev_score_calc_obj_block_size: int, [default=5000000]\n",
      " |      CPU only. Size of block of samples in score calculation. Should be > 0\n",
      " |      Used only for learning speed tuning.\n",
      " |      Changing this parameter can affect results due to numerical accuracy differences\n",
      " |\n",
      " |  dev_efb_max_buckets : int, [default=1024]\n",
      " |      CPU only. Maximum bucket count in exclusive features bundle. Should be in an integer between 0 and 65536.\n",
      " |      Used only for learning speed tuning.\n",
      " |\n",
      " |  sparse_features_conflict_fraction : float, [default=0.0]\n",
      " |      CPU only. Maximum allowed fraction of conflicting non-default values for features in exclusive features bundle.\n",
      " |      Should be a real value in [0, 1) interval.\n",
      " |\n",
      " |  grow_policy : string, [SymmetricTree,Lossguide,Depthwise], [default=SymmetricTree]\n",
      " |      The tree growing policy. It describes how to perform greedy tree construction.\n",
      " |\n",
      " |  min_data_in_leaf : int, [default=1].\n",
      " |      The minimum training samples count in leaf.\n",
      " |      CatBoost will not search for new splits in leaves with samples count less than min_data_in_leaf.\n",
      " |      This parameter is used only for Depthwise and Lossguide growing policies.\n",
      " |\n",
      " |  max_leaves : int, [default=31],\n",
      " |      The maximum leaf count in resulting tree.\n",
      " |      This parameter is used only for Lossguide growing policy.\n",
      " |\n",
      " |  score_function : string, possible values L2, Cosine, NewtonL2, NewtonCosine, [default=Cosine]\n",
      " |      For growing policy Lossguide default=NewtonL2.\n",
      " |      GPU only. Score that is used during tree construction to select the next tree split.\n",
      " |\n",
      " |  max_depth : int, Synonym for depth.\n",
      " |\n",
      " |  n_estimators : int, synonym for iterations.\n",
      " |\n",
      " |  num_trees : int, synonym for iterations.\n",
      " |\n",
      " |  num_boost_round : int, synonym for iterations.\n",
      " |\n",
      " |  colsample_bylevel : float, synonym for rsm.\n",
      " |\n",
      " |  random_state : int, synonym for random_seed.\n",
      " |\n",
      " |  reg_lambda : float, synonym for l2_leaf_reg.\n",
      " |\n",
      " |  objective : string, synonym for loss_function.\n",
      " |\n",
      " |  num_leaves : int, synonym for max_leaves.\n",
      " |\n",
      " |  min_child_samples : int, synonym for min_data_in_leaf\n",
      " |\n",
      " |  eta : float, synonym for learning_rate.\n",
      " |\n",
      " |  max_bin : float, synonym for border_count.\n",
      " |\n",
      " |  scale_pos_weight : float, synonym for class_weights.\n",
      " |      Can be used only for binary classification. Sets weight multiplier for\n",
      " |      class 1 to scale_pos_weight value.\n",
      " |\n",
      " |  metadata : dict, string to string key-value pairs to be stored in model metadata storage\n",
      " |\n",
      " |  early_stopping_rounds : int\n",
      " |      Synonym for od_wait. Only one of these parameters should be set.\n",
      " |\n",
      " |  cat_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Categ features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |\n",
      " |  text_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Text features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |\n",
      " |  embedding_features : list or numpy.ndarray, [default=None]\n",
      " |      If not None, giving the list of Embedding features indices or names (names are represented as strings).\n",
      " |      If it contains feature names, feature names must be defined for the training dataset passed to 'fit'.\n",
      " |\n",
      " |  leaf_estimation_backtracking : string, [default=None]\n",
      " |      Type of backtracking during gradient descent.\n",
      " |      Possible values:\n",
      " |          - 'No' - never backtrack; supported on CPU and GPU\n",
      " |          - 'AnyImprovement' - reduce the descent step until the value of loss function is less than before the step; supported on CPU and GPU\n",
      " |          - 'Armijo' - reduce the descent step until Armijo condition is satisfied; supported on GPU only\n",
      " |\n",
      " |  model_shrink_rate : float, [default=0]\n",
      " |      This parameter enables shrinkage of model at the start of each iteration. CPU only.\n",
      " |      For Constant mode shrinkage coefficient is calculated as (1 - model_shrink_rate * learning_rate).\n",
      " |      For Decreasing mode shrinkage coefficient is calculated as (1 - model_shrink_rate / iteration).\n",
      " |      Shrinkage coefficient should be in [0, 1).\n",
      " |\n",
      " |  model_shrink_mode : string, [default=None]\n",
      " |      Mode of shrinkage coefficient calculation. CPU only.\n",
      " |      Possible values:\n",
      " |          - 'Constant' - Shrinkage coefficient is constant at each iteration.\n",
      " |          - 'Decreasing' - Shrinkage coefficient decreases at each iteration.\n",
      " |\n",
      " |  langevin : bool, [default=False]\n",
      " |      Enables the Stochastic Gradient Langevin Boosting. CPU only.\n",
      " |\n",
      " |  diffusion_temperature : float, [default=0]\n",
      " |      Langevin boosting diffusion temperature. CPU only.\n",
      " |\n",
      " |  posterior_sampling : bool, [default=False]\n",
      " |      Set group of parameters for further use Uncertainty prediction:\n",
      " |          - Langevin = True\n",
      " |          - Model Shrink Rate = 1/(2N), where N is dataset size\n",
      " |          - Model Shrink Mode = Constant\n",
      " |          - Diffusion-temperature = N, where N is dataset size. CPU only.\n",
      " |\n",
      " |  boost_from_average : bool, [default=True for RMSE, False for other losses]\n",
      " |      Enables to initialize approx values by best constant value for specified loss function.\n",
      " |      Available for RMSE, Logloss, CrossEntropy, Quantile and MAE.\n",
      " |\n",
      " |  tokenizers : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'tokenizer_id': 'Tokenizer',  # Tokeinzer identifier.\n",
      " |              'lowercasing': 'false',  # Possible values: 'true', 'false'.\n",
      " |              'number_process_policy': 'LeaveAsIs',  # Possible values: 'Skip', 'LeaveAsIs', 'Replace'.\n",
      " |              'number_token': '%',  # Rarely used character. Used in conjunction with Replace NumberProcessPolicy.\n",
      " |              'separator_type': 'ByDelimiter',  # Possible values: 'ByDelimiter', 'BySense'.\n",
      " |              'delimiter': ' ',  # Used in conjunction with ByDelimiter SeparatorType.\n",
      " |              'split_by_set': 'false',  # Each single character in delimiter used as individual delimiter.\n",
      " |              'skip_empty': 'true',  # Possible values: 'true', 'false'.\n",
      " |              'token_types': ['Word', 'Number', 'Unknown'],  # Used in conjunction with BySense SeparatorType.\n",
      " |                  # Possible values: 'Word', 'Number', 'Punctuation', 'SentenceBreak', 'ParagraphBreak', 'Unknown'.\n",
      " |              'subtokens_policy': 'SingleToken',  # Possible values:\n",
      " |                  # 'SingleToken' - All subtokens are interpreted as single token).\n",
      " |                  # 'SeveralTokens' - All subtokens are interpreted as several token.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |\n",
      " |  dictionaries : list of dicts,\n",
      " |      Each dict is a tokenizer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          {\n",
      " |              'dictionary_id': 'Dictionary',  # Dictionary identifier.\n",
      " |              'token_level_type': 'Word',  # Possible values: 'Word', 'Letter'.\n",
      " |              'gram_order': '1',  # 1 for Unigram, 2 for Bigram, ...\n",
      " |              'skip_step': '0',  # 1 for 1-skip-gram, ...\n",
      " |              'end_of_word_token_policy': 'Insert',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'end_of_sentence_token_policy': 'Skip',  # Possible values: 'Insert', 'Skip'.\n",
      " |              'occurrence_lower_bound': '3',  # The lower bound of token occurrences in the text to include it in the dictionary.\n",
      " |              'max_dictionary_size': '50000',  # The max dictionary size.\n",
      " |          },\n",
      " |          ...\n",
      " |      ]\n",
      " |      ```\n",
      " |\n",
      " |  feature_calcers : list of strings,\n",
      " |      Each string is a calcer description. Example:\n",
      " |      ```\n",
      " |      [\n",
      " |          'NaiveBayes',\n",
      " |          'BM25',\n",
      " |          'BoW:top_tokens_count=2000',\n",
      " |      ]\n",
      " |      ```\n",
      " |\n",
      " |  text_processing : dict,\n",
      " |      Text processging description.\n",
      " |\n",
      " |  eval_fraction : float, [default=None]\n",
      " |      Fraction of the train dataset to be used as the evaluation dataset.\n",
      " |\n",
      " |  Method resolution order:\n",
      " |      CatBoostClassifier\n",
      " |      CatBoost\n",
      " |      _CatBoostBase\n",
      " |      builtins.object\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, iterations=None, learning_rate=None, depth=None, l2_leaf_reg=None, model_size_reg=None, rsm=None, loss_function=None, border_count=None, feature_border_type=None, per_float_feature_quantization=None, input_borders=None, output_borders=None, fold_permutation_block=None, od_pval=None, od_wait=None, od_type=None, nan_mode=None, counter_calc_method=None, leaf_estimation_iterations=None, leaf_estimation_method=None, thread_count=None, random_seed=None, use_best_model=None, best_model_min_trees=None, verbose=None, silent=None, logging_level=None, metric_period=None, ctr_leaf_count_limit=None, store_all_simple_ctr=None, max_ctr_complexity=None, has_time=None, allow_const_label=None, target_border=None, classes_count=None, class_weights=None, auto_class_weights=None, class_names=None, one_hot_max_size=None, random_strength=None, random_score_type=None, name=None, ignored_features=None, train_dir=None, custom_loss=None, custom_metric=None, eval_metric=None, bagging_temperature=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, fold_len_multiplier=None, used_ram_limit=None, gpu_ram_part=None, pinned_memory_size=None, allow_writing_files=None, final_ctr_computation_mode=None, approx_on_full_history=None, boosting_type=None, simple_ctr=None, combinations_ctr=None, per_feature_ctr=None, ctr_description=None, ctr_target_border_count=None, task_type=None, device_config=None, devices=None, bootstrap_type=None, subsample=None, mvs_reg=None, sampling_unit=None, sampling_frequency=None, dev_score_calc_obj_block_size=None, dev_efb_max_buckets=None, sparse_features_conflict_fraction=None, max_depth=None, n_estimators=None, num_boost_round=None, num_trees=None, colsample_bylevel=None, random_state=None, reg_lambda=None, objective=None, eta=None, max_bin=None, scale_pos_weight=None, gpu_cat_features_storage=None, data_partition=None, metadata=None, early_stopping_rounds=None, cat_features=None, grow_policy=None, min_data_in_leaf=None, min_child_samples=None, max_leaves=None, num_leaves=None, score_function=None, leaf_estimation_backtracking=None, ctr_history_unit=None, monotone_constraints=None, feature_weights=None, penalties_coefficient=None, first_feature_use_penalties=None, per_object_feature_penalties=None, model_shrink_rate=None, model_shrink_mode=None, langevin=None, diffusion_temperature=None, posterior_sampling=None, boost_from_average=None, text_features=None, tokenizers=None, dictionaries=None, feature_calcers=None, text_processing=None, embedding_features=None, callback=None, eval_fraction=None, fixed_binary_splits=None)\n",
      " |      Initialize the CatBoost.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      params : dict\n",
      " |          Parameters for CatBoost.\n",
      " |          If  None, all params are set to their defaults.\n",
      " |          If  dict, overriding parameters present in dict.\n",
      " |\n",
      " |  fit(self, X, y=None, cat_features=None, text_features=None, embedding_features=None, graph=None, sample_weight=None, baseline=None, use_best_model=None, eval_set=None, verbose=None, logging_level=None, plot=False, plot_file=None, column_description=None, verbose_eval=None, metric_period=None, silent=None, early_stopping_rounds=None, save_snapshot=None, snapshot_file=None, snapshot_interval=None, init_model=None, callbacks=None, log_cout=None, log_cerr=None)\n",
      " |      Fit the CatBoostClassifier model.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |\n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels of the training data.\n",
      " |          If not None, can be a single- or two- dimensional array with either:\n",
      " |            - numerical values - for binary classification problems\n",
      " |            - class labels (boolean, integer or string)\n",
      " |          Use only if X is not catboost.Pool and does not point to a file.\n",
      " |\n",
      " |      cat_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Categ columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |\n",
      " |      text_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Text columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |\n",
      " |      embedding_features : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving the list of Embedding columns indices.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |\n",
      " |      graph : list or numpy.ndarray or pandas.DataFrame\n",
      " |          The graph edges list description.\n",
      " |          If list or numpy.ndarrays or pandas.DataFrame, giving 2 dimensional.\n",
      " |\n",
      " |      sample_weight : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Instance weights, 1 dimensional array like.\n",
      " |\n",
      " |      baseline : list or numpy.ndarray, optional (default=None)\n",
      " |          If not None, giving 2 dimensional array like data.\n",
      " |          Use only if X is not catboost.Pool.\n",
      " |\n",
      " |      use_best_model : bool, optional (default=None)\n",
      " |          Flag to use best model\n",
      " |\n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |\n",
      " |      metric_period : int\n",
      " |          Frequency of evaluating metrics.\n",
      " |\n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |\n",
      " |      silent : bool\n",
      " |          If silent is True, logging_level is set to Silent.\n",
      " |          If silent is False, logging_level is set to Verbose.\n",
      " |\n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |\n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |\n",
      " |      verbose_eval : bool or int\n",
      " |          Synonym for verbose. Only one of these parameters should be set.\n",
      " |\n",
      " |      early_stopping_rounds : int\n",
      " |          Activates Iter overfitting detector with od_wait set to early_stopping_rounds.\n",
      " |\n",
      " |      save_snapshot : bool, [default=None]\n",
      " |          Enable progress snapshotting for restoring progress after crashes or interruptions\n",
      " |\n",
      " |      snapshot_file : string or pathlib.Path, [default=None]\n",
      " |          Learn progress snapshot file path, if None will use default filename\n",
      " |\n",
      " |      snapshot_interval: int, [default=600]\n",
      " |          Interval between saving snapshots (seconds)\n",
      " |\n",
      " |      init_model : CatBoost class or string or pathlib.Path, [default=None]\n",
      " |          Continue training starting from the existing model.\n",
      " |          If this parameter is a string or pathlib.Path, load initial model from the path specified by this string.\n",
      " |\n",
      " |      callbacks : list, optional (default=None)\n",
      " |          List of callback objects that are applied at end of each iteration.\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      model : CatBoost\n",
      " |\n",
      " |  get_probability_threshold(self)\n",
      " |      Get a threshold for class separation in binary classification task\n",
      " |\n",
      " |  predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict with data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      task_type : string, [default=None]\n",
      " |          The evaluator type.\n",
      " |          Possible values:\n",
      " |              - 'CPU'\n",
      " |              - 'GPU' (models with only numerical features are supported for now)\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return class label.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |\n",
      " |  predict_log_proba(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class log probability with data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      task_type : string, [default=None]\n",
      " |          The evaluator type.\n",
      " |          Possible values:\n",
      " |              - 'CPU'\n",
      " |              - 'GPU' (models with only numerical features are supported for now)\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |\n",
      " |  predict_proba(self, X, ntree_start=0, ntree_end=0, thread_count=-1, verbose=None, task_type='CPU')\n",
      " |      Predict class probability with X.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If X is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      task_type : string, [default=None]\n",
      " |          The evaluator type.\n",
      " |          Possible values:\n",
      " |              - 'CPU'\n",
      " |              - 'GPU' (models with only numerical features are supported for now)\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          If X is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |\n",
      " |  score(self, X, y=None)\n",
      " |      Calculate accuracy.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          Data to apply model on.\n",
      " |      y : list or numpy.ndarray\n",
      " |          True labels.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      accuracy : float\n",
      " |\n",
      " |  set_probability_threshold(self, binclass_probability_threshold=None)\n",
      " |      Set a threshold for class separation in binary classification task for a trained model.\n",
      " |      :param binclass_probability_threshold: float number in [0, 1] or None to discard it\n",
      " |\n",
      " |  staged_predict(self, data, prediction_type='Class', ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict target at each stage for data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      prediction_type : string, optional (default='Class')\n",
      " |          Can be:\n",
      " |          - 'RawFormulaVal' : return raw formula value.\n",
      " |          - 'Class' : return class label.\n",
      " |          - 'Probability' : return probability for every class.\n",
      " |          - 'LogProbability' : return log probability for every class.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object, the return value depends on prediction_type value:\n",
      " |              - 'RawFormulaVal' : return raw formula value.\n",
      " |              - 'Class' : return majority vote class.\n",
      " |              - 'Probability' : return one-dimensional numpy.ndarray with probability for every class.\n",
      " |              - 'LogProbability' : return one-dimensional numpy.ndarray with\n",
      " |                log probability for every class.\n",
      " |          otherwise numpy.ndarray, with values that depend on prediction_type value:\n",
      " |              - 'RawFormulaVal' : one-dimensional array of raw formula value for each object.\n",
      " |              - 'Class' : one-dimensional array of class label for each object.\n",
      " |              - 'Probability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with probability for every class for each object.\n",
      " |              - 'LogProbability' : two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |                with log probability for every class for each object.\n",
      " |\n",
      " |  staged_predict_log_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with log probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with log probability for every class for each object.\n",
      " |\n",
      " |  staged_predict_proba(self, data, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, verbose=None)\n",
      " |      Predict classification target at each stage for data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : generator for each iteration that generates:\n",
      " |          If data is for a single object\n",
      " |              return one-dimensional numpy.ndarray with probability for every class.\n",
      " |          otherwise\n",
      " |              return two-dimensional numpy.ndarray with shape (number_of_objects x number_of_classes)\n",
      " |              with probability for every class for each object.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from CatBoost:\n",
      " |\n",
      " |  calc_feature_statistics(self, data, target=None, feature=None, prediction_type=None, cat_feature_values=None, plot=True, max_cat_features_on_plot=10, thread_count=-1, plot_file=None)\n",
      " |      Get statistics for the feature using the model, dataset and target.\n",
      " |      To use this function, you should install plotly.\n",
      " |\n",
      " |      The catboost model has borders for the float features used in it. The borders divide\n",
      " |      feature values into bins, and the model's prediction depends on the number of the bin where the\n",
      " |      feature value falls in.\n",
      " |\n",
      " |      For float features this function takes model's borders and computes\n",
      " |      1) Mean target value for every bin;\n",
      " |      2) Mean model prediction for every bin;\n",
      " |      3) The number of objects in dataset which fall into each bin;\n",
      " |      4) Predictions on varying feature. For every object, varies the feature value\n",
      " |      so that it falls into bin #0, bin #1, ... and counts model predictions.\n",
      " |      Then counts average prediction for each bin.\n",
      " |\n",
      " |      For categorical features (only one-hot supported) does the same, but takes feature values\n",
      " |      provided in cat_feature_values instead of borders.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost. Pool or dict {'pool_name': pool} if you want several pools\n",
      " |          Data to compute statistics on\n",
      " |      target: numpy.ndarray or pandas.Series or dict {'pool_name': target} if you want several pools or None\n",
      " |          Target corresponding to data\n",
      " |          Use only if data is not catboost.Pool.\n",
      " |      feature: None, int, string, or list of int or strings\n",
      " |          Features indexes or names in pd.DataFrame for which you want to get statistics.\n",
      " |          None, if you need statistics for all features.\n",
      " |      prediction_type: str\n",
      " |          Prediction type used for counting mean_prediction: 'Class', 'Probability' or 'RawFormulaVal'.\n",
      " |          If not specified, is derived from the model.\n",
      " |      cat_feature_values: list or numpy.ndarray or pandas.Series or\n",
      " |                          dict: int or string to list or numpy.ndarray or pandas.Series\n",
      " |          Contains categorical feature values you need to get statistics on.\n",
      " |          Use dict, when parameter 'feature' is a list to specify cat values for different features.\n",
      " |          When parameter 'feature' is int or str, you can just pass list of cat values.\n",
      " |      plot: bool\n",
      " |          Plot statistics.\n",
      " |      max_cat_features_on_plot: int\n",
      " |          If categorical feature takes more than max_cat_features_on_plot different unique values,\n",
      " |          output result on several plots, not more than max_cat_features_on_plot feature values on each.\n",
      " |          Used only if plot=True or plot_file is not None.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use for getting statistics.\n",
      " |      plot_file: str\n",
      " |          Output file for plot statistics.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict if parameter 'feature' is int or string, else dict of dicts:\n",
      " |          For each unique feature contain\n",
      " |          python dict with binarized feature statistics.\n",
      " |          For float feature, includes\n",
      " |                  'borders' -- borders for the specified feature in model\n",
      " |                  'binarized_feature' -- numbers of bins where feature values fall\n",
      " |                  'mean_target' -- mean value of target over each bin\n",
      " |                  'mean_prediction' -- mean value of model prediction over each bin\n",
      " |                  'objects_per_bin' -- number of objects per bin\n",
      " |                  'predictions_on_varying_feature' -- averaged over dataset predictions for\n",
      " |                  varying feature (see above)\n",
      " |          For one-hot feature, returns the same, but with 'cat_values' instead of 'borders'\n",
      " |\n",
      " |  calc_leaf_indexes(self, data, ntree_start=0, ntree_end=0, thread_count=-1, verbose=False)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool (default=False)\n",
      " |          Enable debug logging level.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : 2-dimensional numpy.ndarray of numpy.uint32 with shape (object count, ntree_end - ntree_start).\n",
      " |          i-th row is an array of leaf indexes for i-th object.\n",
      " |\n",
      " |  compare(self, model, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot_file=None, log_cout=None, log_cerr=None)\n",
      " |      Draw train and eval errors in Jupyter notebook for both models\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      model: CatBoost model\n",
      " |          Another model to draw metrics\n",
      " |\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |\n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save eval error graphs to file\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |  create_metric_calcer(self, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None)\n",
      " |      Create batch metric calcer. Could be used to aggregate metric on several pools\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |          Same as in eval_metrics except data\n",
      " |      Returns\n",
      " |      -------\n",
      " |          BatchMetricCalcer object\n",
      " |\n",
      " |      Usage example\n",
      " |      -------\n",
      " |      # Large dataset is partitioned into parts [part1, part2]\n",
      " |      model.fit(params)\n",
      " |      batch_calcer = model.create_metric_calcer(['Logloss'])\n",
      " |      batch_calcer.add(part1)\n",
      " |      batch_calcer.add(part2)\n",
      " |      metrics = batch_calcer.eval_metrics()\n",
      " |\n",
      " |  drop_unused_features(self)\n",
      " |      Drop unused features information from model\n",
      " |\n",
      " |  eval_metrics(self, data, metrics, ntree_start=0, ntree_end=0, eval_period=1, thread_count=-1, tmp_dir=None, plot=False, plot_file=None, log_cout=None, log_cerr=None)\n",
      " |      Calculate metrics.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool\n",
      " |          Data to evaluate metrics on.\n",
      " |\n",
      " |      metrics : list of strings or catboost.metrics.BuiltinMetric\n",
      " |          List of evaluated metrics.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      eval_period: int, optional (default=1)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) with the step eval_period (zero-based indexing).\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      tmp_dir : string or pathlib.Path (default=None)\n",
      " |          The name of the temporary directory for intermediate results.\n",
      " |          If None, then the name will be generated.\n",
      " |\n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction : dict: metric -> array of shape [(ntree_end - ntree_start) / eval_period]\n",
      " |\n",
      " |  get_all_params(self)\n",
      " |      Get all params (specified by user and default params) that were set in training from CatBoost model.\n",
      " |      Full parameters documentation could be found here: https://catboost.ai/docs/concepts/python-reference_parameters-list.html\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |\n",
      " |  get_borders(self)\n",
      " |      Return map feature_index: borders for float features.\n",
      " |\n",
      " |  get_cat_feature_indices(self)\n",
      " |\n",
      " |  get_embedding_feature_indices(self)\n",
      " |\n",
      " |  get_feature_importance(self, data=None, type=<EFstrType.FeatureImportance: 2>, prettified=False, thread_count=-1, verbose=False, fstr_type=None, shap_mode='Auto', model_output='Raw', interaction_indices=None, shap_calc_type='Regular', reference_data=None, sage_n_samples=128, sage_batch_size=512, sage_detect_convergence=True, log_cout=None, log_cerr=None)\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data :\n",
      " |          Data to get feature importance.\n",
      " |          If type in ('LossFunctionChange', 'ShapValues', 'ShapInteractionValues') data must of Pool type.\n",
      " |              For every object in this dataset feature importances will be calculated.\n",
      " |          if type == 'SageValues' data must of Pool type.\n",
      " |              For every feature in this dataset importance will be calculated.\n",
      " |          If type == 'PredictionValuesChange', data is None or a dataset of Pool type\n",
      " |              Dataset specification is needed only in case if the model does not contain leaf weight information (trained with CatBoost v < 0.9).\n",
      " |          If type == 'PredictionDiff' data must contain a matrix of feature values of shape (2, n_features).\n",
      " |              Possible types are catboost.Pool or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData or pandas.SparseDataFrame or scipy.sparse.spmatrix\n",
      " |          If type == 'FeatureImportance'\n",
      " |              See 'PredictionValuesChange' for non-ranking metrics and 'LossFunctionChange' for ranking metrics.\n",
      " |          If type == 'Interaction'\n",
      " |              This parameter is not used.\n",
      " |\n",
      " |      type : EFstrType or string (converted to EFstrType), optional\n",
      " |                  (default=EFstrType.FeatureImportance)\n",
      " |          Possible values:\n",
      " |              - PredictionValuesChange\n",
      " |                  Calculate score for every feature.\n",
      " |              - LossFunctionChange\n",
      " |                  Calculate score for every feature by loss.\n",
      " |              - FeatureImportance\n",
      " |                  PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics\n",
      " |              - ShapValues\n",
      " |                  Calculate SHAP Values for every object.\n",
      " |              - ShapInteractionValues\n",
      " |                  Calculate SHAP Interaction Values between each pair of features for every object\n",
      " |              - Interaction\n",
      " |                  Calculate pairwise score between every feature.\n",
      " |              - PredictionDiff\n",
      " |                  Calculate most important features explaining difference in predictions for a pair of documents.\n",
      " |              - SageValues\n",
      " |                  Calculate SAGE value for every feature\n",
      " |\n",
      " |      prettified : bool, optional (default=False)\n",
      " |          change returned data format to the list of (feature_id, importance) pairs sorted by importance\n",
      " |\n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |\n",
      " |      fstr_type : string, deprecated, use type instead\n",
      " |\n",
      " |      shap_mode : string, optional (default=\"Auto\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Auto\"\n",
      " |                  Use direct SHAP Values calculation only if data size is smaller than average leaves number\n",
      " |                  (the best of two strategies below is chosen).\n",
      " |              - \"UsePreCalc\"\n",
      " |                  Calculate SHAP Values for every leaf in preprocessing. Final complexity is\n",
      " |                  O(NT(D+F))+O(TL^2 D^2) where N is the number of documents(objects), T - number of trees,\n",
      " |                  D - average tree depth, F - average number of features in tree, L - average number of leaves in tree\n",
      " |                  This is much faster (because of a smaller constant) than direct calculation when N >> L\n",
      " |              - \"NoPreCalc\"\n",
      " |                  Use direct SHAP Values calculation calculation with complexity O(NTLD^2). Direct algorithm\n",
      " |                  is faster when N < L (algorithm from https://arxiv.org/abs/1802.03888)\n",
      " |\n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=\"Regular\")\n",
      " |          used only for ShapValues type\n",
      " |          Possible values:\n",
      " |              - \"Regular\"\n",
      " |                  Calculate regular SHAP values\n",
      " |              - \"Approximate\"\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - \"Exact\"\n",
      " |                  Calculate exact SHAP values\n",
      " |\n",
      " |      interaction_indices : list of int or string (feature_idx_1, feature_idx_2), optional (default=None)\n",
      " |          used only for ShapInteractionValues type\n",
      " |          Calculate SHAP Interaction Values between pair of features feature_idx_1 and feature_idx_2 for every object\n",
      " |\n",
      " |      reference_data: catboost.Pool or None\n",
      " |          Reference data for Independent Tree SHAP values from https://arxiv.org/abs/1905.04610v1\n",
      " |          if type == 'ShapValues' and reference_data is not None, then Independent Tree SHAP values are calculated\n",
      " |\n",
      " |      sage_n_samples: int, optional (default=32)\n",
      " |          Number of outer samples used in SAGE values approximation algorithm\n",
      " |      sage_batch_size: int, optional (default=min(512, number of samples in dataset))\n",
      " |          Number of samples used on each step of SAGE values approximation algorithm\n",
      " |      sage_detect_convergence: bool, optional (default=False)\n",
      " |          If set True, sage values calculation will be stopped either when sage values converge\n",
      " |          or when sage_n_samples iterations of algorithm pass\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      depends on type:\n",
      " |          - FeatureImportance\n",
      " |              See PredictionValuesChange for non-ranking metrics and LossFunctionChange for ranking metrics.\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=False (default)\n",
      " |              list of length [n_features] with feature_importance values (float) for feature\n",
      " |          - PredictionValuesChange, LossFunctionChange, PredictionDiff, SageValues with prettified=True\n",
      " |              list of length [n_features] with (feature_id (string), feature_importance (float)) pairs, sorted by feature_importance in descending order\n",
      " |          - ShapValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1) with Shap values (float) for (object, feature).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1). For each object it contains Shap values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - ShapInteractionValues\n",
      " |              np.ndarray of shape (n_objects, n_features + 1, n_features + 1) with Shap interaction values (float) for (object, feature(i), feature(j)).\n",
      " |              In case of multiclass the returned value is np.ndarray of shape\n",
      " |              (n_objects, classes_count, n_features + 1, n_features + 1). For each object it contains Shap interaction values (float).\n",
      " |              Values are calculated for RawFormulaVal predictions.\n",
      " |          - Interaction\n",
      " |              list of length [n_features] of 3-element lists of (first_feature_index, second_feature_index, interaction_score (float))\n",
      " |\n",
      " |  get_object_importance(self, pool, train_pool, top_size=-1, type='Average', update_method='SinglePoint', importance_values_sign='All', thread_count=-1, verbose=False, ostr_type=None, log_cout=None, log_cerr=None)\n",
      " |      This is the implementation of the LeafInfluence algorithm from the following paper:\n",
      " |      https://arxiv.org/pdf/1802.06640.pdf\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      pool : Pool\n",
      " |          The pool for which you want to evaluate the object importances.\n",
      " |\n",
      " |      train_pool : Pool\n",
      " |          The pool on which the model has been trained.\n",
      " |\n",
      " |      top_size : int (default=-1)\n",
      " |          Method returns the result of the top_size most important train objects.\n",
      " |          If -1, then the top size is not limited.\n",
      " |\n",
      " |      type : string, optional (default='Average')\n",
      " |          Possible values:\n",
      " |              - Average (Method returns the mean train objects scores for all input objects)\n",
      " |              - PerObject (Method returns the train objects scores for every input object)\n",
      " |\n",
      " |      importance_values_sign : string, optional (default='All')\n",
      " |          Method returns only Positive, Negative or All values.\n",
      " |          Possible values:\n",
      " |              - Positive\n",
      " |              - Negative\n",
      " |              - All\n",
      " |\n",
      " |      update_method : string, optional (default='SinglePoint')\n",
      " |          Possible values:\n",
      " |              - SinglePoint\n",
      " |              - TopKLeaves (It is posible to set top size : TopKLeaves:top=2)\n",
      " |              - AllPoints\n",
      " |          Description of the update set methods are given in section 3.1.3 of the paper.\n",
      " |\n",
      " |      thread_count : int, optional (default=-1)\n",
      " |          Number of threads.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool or int\n",
      " |          If False, then evaluation is not logged. If True, then each possible iteration is logged.\n",
      " |          If a positive integer, then it stands for the size of batch N. After processing each batch, print progress\n",
      " |          and remaining time.\n",
      " |\n",
      " |      ostr_type : string, deprecated, use type instead\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      object_importances : tuple of two arrays (indices and scores) of shape = [top_size]\n",
      " |\n",
      " |  get_param(self, key)\n",
      " |      Get param value from CatBoost model.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      key : string\n",
      " |          The key to get param value from.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      value :\n",
      " |          The param value of the key, returns None if param do not exist.\n",
      " |\n",
      " |  get_params(self, deep=True)\n",
      " |      Get all params from CatBoost model.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      result : dict\n",
      " |          Dictionary of {param_key: param_value}.\n",
      " |\n",
      " |  get_text_feature_indices(self)\n",
      " |\n",
      " |  grid_search(self, param_grid, X, y=None, cv=3, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=None, log_cerr=None)\n",
      " |      Exhaustive search over specified parameter values for a model.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_grid: dict or list of dictionaries\n",
      " |          Dictionary with parameters names (string) as keys and lists of parameter settings\n",
      " |          to try as values, or a list of such dictionaries, in which case the grids spanned by each\n",
      " |          dictionary in the list are explored.\n",
      " |          This enables searching over any sequence of parameter settings.\n",
      " |\n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |\n",
      " |      y: list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels of the training data.\n",
      " |          If not None, can be a single- or two- dimensional array with either:\n",
      " |            - numerical values - for regression (including multiregression), ranking and binary classification problems\n",
      " |            - class labels (boolean, integer or string) - for classification (including multiclassification) problems\n",
      " |          Use only if X is not catboost.Pool and does not point to a file.\n",
      " |\n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |\n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |\n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |\n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |\n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |\n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |\n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |\n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for final cross-validation.\n",
      " |\n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |\n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |\n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |\n",
      " |  iterate_leaf_indexes(self, data, ntree_start=0, ntree_end=0)\n",
      " |      Returns indexes of leafs to which objects from pool are mapped by model trees.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Index of first tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Index of the tree after last tree for which leaf indexes will be calculated (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_indexes : generator. For each object in pool yields one-dimensional numpy.ndarray of leaf indexes.\n",
      " |\n",
      " |  load_model(self, fname=None, format='cbm', stream=None, blob=None)\n",
      " |      Load model from a file, stream or blob.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Input file name.\n",
      " |\n",
      " |  plot_partial_dependence(self, data, features, plot=True, plot_file=None, thread_count=-1)\n",
      " |      To use this function, you should install plotly.\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features: int, str, list<int>, tuple<int>, list<string>, tuple<string>\n",
      " |          Float features to calculate partial dependence for. Number of features should be 1 or 2.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      thread_count: int\n",
      " |          Number of threads to use. If -1 use maximum available number of threads.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          If number of features is one - 1d numpy array and figure with line plot.\n",
      " |          If number of features is two - 2d numpy array and figure with 2d heatmap.\n",
      " |\n",
      " |  plot_predictions(self, data, features_to_change, plot=True, plot_file=None)\n",
      " |      To use this function, you should install plotly.\n",
      " |\n",
      " |      data: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |      features_to_change: list-like with int (for indices) or str (for names) elements\n",
      " |          Numerical features indices or names in `data` for which you want to vary prediction value.\n",
      " |      plot: bool\n",
      " |          Plot predictions.\n",
      " |      plot_file: str\n",
      " |          Output file for plot predictions.\n",
      " |      Returns\n",
      " |      -------\n",
      " |          List of list of predictions for all buckets for all samples in data\n",
      " |\n",
      " |  plot_tree(self, tree_idx, pool=None)\n",
      " |\n",
      " |  randomized_search(self, param_distributions, X, y=None, cv=3, n_iter=10, partition_random_seed=0, calc_cv_statistics=True, search_by_train_test_split=True, refit=True, shuffle=True, stratified=None, train_size=0.8, verbose=True, plot=False, plot_file=None, log_cout=None, log_cerr=None)\n",
      " |      Randomized search on hyper parameters.\n",
      " |      After calling this method model is fitted and can be used, if not specified otherwise (refit=False).\n",
      " |\n",
      " |      In contrast to grid_search, not all parameter values are tried out,\n",
      " |      but rather a fixed number of parameter settings is sampled from the specified distributions.\n",
      " |      The number of parameter settings that are tried is given by n_iter.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      param_distributions: dict\n",
      " |          Dictionary with parameters names (string) as keys and distributions or lists of parameters to try.\n",
      " |          Distributions must provide a rvs method for sampling (such as those from scipy.stats.distributions).\n",
      " |          If a list is given, it is sampled uniformly.\n",
      " |\n",
      " |      X: numpy.ndarray or pandas.DataFrame or catboost.Pool\n",
      " |          Data to compute statistics on\n",
      " |\n",
      " |      y: list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels of the training data.\n",
      " |          If not None, can be a single- or two- dimensional array with either:\n",
      " |            - numerical values - for regression (including multiregression), ranking and binary classification problems\n",
      " |            - class labels (boolean, integer or string) - for classification (including multiclassification) problems\n",
      " |          Use only if X is not catboost.Pool and does not point to a file.\n",
      " |\n",
      " |      cv: int, cross-validation generator or an iterable, optional (default=None)\n",
      " |          Determines the cross-validation splitting strategy. Possible inputs for cv are:\n",
      " |          - None, to use the default 3-fold cross validation,\n",
      " |          - integer, to specify the number of folds in a (Stratified)KFold\n",
      " |          - one of the scikit-learn splitter classes\n",
      " |              (https://scikit-learn.org/stable/modules/classes.html#splitter-classes)\n",
      " |          - An iterable yielding (train, test) splits as arrays of indices.\n",
      " |\n",
      " |      n_iter: int\n",
      " |          Number of parameter settings that are sampled.\n",
      " |          n_iter trades off runtime vs quality of the solution.\n",
      " |\n",
      " |      partition_random_seed: int, optional (default=0)\n",
      " |          Use this as the seed value for random permutation of the data.\n",
      " |          Permutation is performed before splitting the data for cross validation.\n",
      " |          Each seed generates unique data splits.\n",
      " |          Used only when cv is None or int.\n",
      " |\n",
      " |      search_by_train_test_split: bool, optional (default=True)\n",
      " |          If True, source dataset is splitted into train and test parts, models are trained\n",
      " |          on the train part and parameters are compared by loss function score on the test part.\n",
      " |          After that, if calc_cv_statistics=true, statistics on metrics are calculated\n",
      " |          using cross-validation using best parameters and the model is fitted with these parameters.\n",
      " |\n",
      " |          If False, every iteration of grid search evaluates results on cross-validation.\n",
      " |          It is recommended to set parameter to True for large datasets, and to False for small datasets.\n",
      " |\n",
      " |      calc_cv_statistics: bool, optional (default=True)\n",
      " |          The parameter determines whether quality should be estimated.\n",
      " |          using cross-validation with the found best parameters. Used only when search_by_train_test_split=True.\n",
      " |\n",
      " |      refit: bool (default=True)\n",
      " |          Refit an estimator using the best found parameters on the whole dataset.\n",
      " |\n",
      " |      shuffle: bool, optional (default=True)\n",
      " |          Shuffle the dataset objects before parameters searching.\n",
      " |\n",
      " |      stratified: bool, optional (default=None)\n",
      " |          Perform stratified sampling. True for classification and False otherwise.\n",
      " |          Currently supported only for cross-validation.\n",
      " |\n",
      " |      train_size: float, optional (default=0.8)\n",
      " |          Should be between 0.0 and 1.0 and represent the proportion of the dataset to include in the train split.\n",
      " |\n",
      " |      verbose: bool or int, optional (default=True)\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output\n",
      " |          verbose==True is equal to verbose==1\n",
      " |          When verbose==False, there is no messages\n",
      " |\n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error for every set of parameters in Jupyter notebook\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error for every set of parameters to file\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with two fields:\n",
      " |          'params': dict of best found parameters\n",
      " |          'cv_results': dict or pandas.core.frame.DataFrame with cross-validation results\n",
      " |              columns are: test-error-mean  test-error-std  train-error-mean  train-error-std\n",
      " |\n",
      " |  save_borders(self, fname)\n",
      " |      Save the model borders to a file.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string or pathlib.Path\n",
      " |          Output file name.\n",
      " |\n",
      " |  save_model(self, fname, format='cbm', export_parameters=None, pool=None)\n",
      " |      Save the model to a file.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      fname : string\n",
      " |          Output file name.\n",
      " |      format : string\n",
      " |          Possible values:\n",
      " |              * 'cbm' for catboost binary format,\n",
      " |              * 'coreml' to export into Apple CoreML format\n",
      " |              * 'onnx' to export into ONNX-ML format\n",
      " |              * 'pmml' to export into PMML format\n",
      " |              * 'cpp' to export as C++ code\n",
      " |              * 'python' to export as Python code.\n",
      " |      export_parameters : dict\n",
      " |          Parameters for CoreML export:\n",
      " |              * prediction_type : string - either 'probability' or 'raw'\n",
      " |              * coreml_description : string\n",
      " |              * coreml_model_version : string\n",
      " |              * coreml_model_author : string\n",
      " |              * coreml_model_license: string\n",
      " |          Parameters for PMML export:\n",
      " |              * pmml_copyright : string\n",
      " |              * pmml_description : string\n",
      " |              * pmml_model_version : string\n",
      " |      pool : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series or catboost.FeaturesData\n",
      " |          Training pool.\n",
      " |\n",
      " |  select_features(self, X, y=None, eval_set=None, features_for_select=None, num_features_to_select=None, algorithm=None, steps=None, shap_calc_type=None, train_final_model=True, verbose=None, logging_level=None, plot=False, plot_file=None, log_cout=None, log_cerr=None, grouping=None, features_tags_for_select=None, num_features_tags_to_select=None)\n",
      " |      Select best features from pool according to loss value.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : catboost.Pool or list or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |          If not catboost.Pool, 2 dimensional Feature matrix or string - file with dataset.\n",
      " |\n",
      " |      y : list or numpy.ndarray or pandas.DataFrame or pandas.Series, optional (default=None)\n",
      " |          Labels of the training data.\n",
      " |          If not None, can be a single- or two- dimensional array with either:\n",
      " |            - numerical values - for regression (including multiregression), ranking and binary classification problems\n",
      " |            - class labels (boolean, integer or string) - for classification (including multiclassification) problems\n",
      " |          Use only if X is not catboost.Pool and does not point to a file.\n",
      " |\n",
      " |      eval_set : catboost.Pool or list of catboost.Pool or tuple (X, y) or list [(X, y)], optional (default=None)\n",
      " |          Validation dataset or datasets for metrics calculation and possibly early stopping.\n",
      " |\n",
      " |      features_for_select : str or list of feature indices, names or ranges\n",
      " |          (for grouping = Individual)\n",
      " |          Which features should participate in the selection.\n",
      " |          Format examples:\n",
      " |              - [0, 2, 3, 4, 17]\n",
      " |              - [0, \"2-4\", 17] (both ends in ranges are inclusive)\n",
      " |              - \"0,2-4,20\"\n",
      " |              - [\"Name0\", \"Name2\", \"Name3\", \"Name4\", \"Name20\"]\n",
      " |\n",
      " |      num_features_to_select : positive int\n",
      " |          (for grouping = Individual)\n",
      " |          How many features to select from features_for_select.\n",
      " |\n",
      " |      algorithm : EFeaturesSelectionAlgorithm or string, optional (default=RecursiveByShapValues)\n",
      " |          Which algorithm to use for features selection.\n",
      " |          Possible values:\n",
      " |              - RecursiveByPredictionValuesChange\n",
      " |                  Use prediction values change as feature strength, eliminate batch of features at once.\n",
      " |              - RecursiveByLossFunctionChange\n",
      " |                  Use loss function change as feature strength, eliminate batch of features at each step.\n",
      " |              - RecursiveByShapValues\n",
      " |                  Use shap values to estimate loss function change, eliminate features one by one.\n",
      " |\n",
      " |      steps : positive int, optional (default=1)\n",
      " |          How many steps should be performed. In other words, how many times a full model will be trained.\n",
      " |          More steps give more accurate results.\n",
      " |\n",
      " |      shap_calc_type : EShapCalcType or string, optional (default=Regular)\n",
      " |          Which method to use for calculation of shap values.\n",
      " |          Possible values:\n",
      " |              - Regular\n",
      " |                  Calculate regular SHAP values\n",
      " |              - Approximate\n",
      " |                  Calculate approximate SHAP values\n",
      " |              - Exact\n",
      " |                  Calculate exact SHAP values\n",
      " |\n",
      " |      train_final_model : bool, optional (default=True)\n",
      " |          Need to fit model with selected features.\n",
      " |\n",
      " |      verbose : bool or int\n",
      " |          If verbose is bool, then if set to True, logging_level is set to Verbose,\n",
      " |          if set to False, logging_level is set to Silent.\n",
      " |          If verbose is int, it determines the frequency of writing metrics to output and\n",
      " |          logging_level is set to Verbose.\n",
      " |\n",
      " |      logging_level : string, optional (default=None)\n",
      " |          Possible values:\n",
      " |              - 'Silent'\n",
      " |              - 'Verbose'\n",
      " |              - 'Info'\n",
      " |              - 'Debug'\n",
      " |\n",
      " |      plot : bool, optional (default=False)\n",
      " |          If True, draw train and eval error in Jupyter notebook.\n",
      " |\n",
      " |      plot_file : file-like or str, optional (default=None)\n",
      " |          If not None, save train and eval error graphs to file\n",
      " |\n",
      " |      log_cout: output stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stdout is used\n",
      " |\n",
      " |      log_cerr: error stream or callback for logging (default=None)\n",
      " |          If None is specified, sys.stderr is used\n",
      " |\n",
      " |      grouping : EFeaturesSelectionGrouping or string, optional (default=Individual)\n",
      " |          Which grouping to use for features selection.\n",
      " |          Possible values:\n",
      " |              - Individual\n",
      " |                  Select individual features\n",
      " |              - ByTags\n",
      " |                  Select feature groups (marked by tags)\n",
      " |\n",
      " |      features_tags_for_select : list of strings\n",
      " |          (for grouping = ByTags)\n",
      " |          Which features tags should participate in the selection.\n",
      " |\n",
      " |      num_features_tags_to_select : positive int\n",
      " |          (for grouping = ByTags)\n",
      " |          How many features tags to select from features_tags_for_select.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      dict with fields:\n",
      " |          'selected_features': list of selected features indices\n",
      " |          'eliminated_features': list of eliminated features indices\n",
      " |          'selected_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |          'eliminated_features_tags': list of selected features tags (optional, present if grouping == ByTags)\n",
      " |\n",
      " |  set_params(self, **params)\n",
      " |      Set parameters into CatBoost model.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : key=value format\n",
      " |          List of key=value paris. Example: model.set_params(iterations=500, thread_count=2).\n",
      " |\n",
      " |  shrink(self, ntree_end, ntree_start=0)\n",
      " |      Shrink the model.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      ntree_end: int\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |      ntree_start: int, optional (default=0)\n",
      " |          Leave the trees with indices from the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |\n",
      " |  virtual_ensembles_predict(self, data, prediction_type='VirtEnsembles', ntree_end=0, virtual_ensembles_count=10, thread_count=-1, verbose=None)\n",
      " |      Predict with data.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      data : catboost.Pool or list of features or list of lists or numpy.ndarray or pandas.DataFrame or pandas.Series\n",
      " |              or catboost.FeaturesData\n",
      " |          Data to apply model on.\n",
      " |          If data is a simple list (not list of lists) or a one-dimensional numpy.ndarray it is interpreted\n",
      " |          as a list of features for a single object.\n",
      " |\n",
      " |      prediction_type : string, optional (default='RawFormulaVal')\n",
      " |          Can be:\n",
      " |          - 'VirtEnsembles': return V (virtual_ensembles_count) predictions.\n",
      " |              k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant.\n",
      " |          - 'TotalUncertainty': see returned predictions format in 'Returns' part\n",
      " |\n",
      " |      ntree_end: int, optional (default=0)\n",
      " |          Model is applied on the interval [ntree_start, ntree_end) (zero-based indexing).\n",
      " |          If value equals to 0 this parameter is ignored and ntree_end equal to tree_count_.\n",
      " |\n",
      " |      virtual_ensembles_count: int, optional (default=10)\n",
      " |          virtual ensembles count for 'TotalUncertainty' and 'VirtEnsembles' prediction types.\n",
      " |\n",
      " |      thread_count : int (default=-1)\n",
      " |          The number of threads to use when applying the model.\n",
      " |          Allows you to optimize the speed of execution. This parameter doesn't affect results.\n",
      " |          If -1, then the number of threads is set to the number of CPU cores.\n",
      " |\n",
      " |      verbose : bool, optional (default=False)\n",
      " |          If True, writes the evaluation metric measured set to stderr.\n",
      " |\n",
      " |      Returns\n",
      " |      -------\n",
      " |      prediction :\n",
      " |          (with V as virtual_ensembles_count and T as trees count,\n",
      " |          k-th virtEnsemle consists of trees [0, T/2] + [T/2 + T/(2V) * k, T/2 + T/(2V) * (k + 1)]  * constant)\n",
      " |          If data is for a single object, return 1-dimensional array of predictions with size depends on prediction type,\n",
      " |          otherwise return 2-dimensional numpy.ndarray with shape (number_of_objects x size depends on prediction type);\n",
      " |          Returned predictions depends on prediction type:\n",
      " |          If loss-function was RMSEWithUncertainty:\n",
      " |              - 'VirtEnsembles': [mean0, var0, mean1, var1, ..., vark-1].\n",
      " |              - 'TotalUncertainty': [mean_predict, KnowledgeUnc, DataUnc].\n",
      " |          otherwise for regression:\n",
      " |              - 'VirtEnsembles':  [mean0, mean1, ...].\n",
      " |              - 'TotalUncertainty': [mean_predicts, KnowledgeUnc].\n",
      " |          otherwise for binary classification:\n",
      " |              - 'VirtEnsembles':  [ApproxRawFormulaVal0, ApproxRawFormulaVal1, ..., ApproxRawFormulaValk-1].\n",
      " |              - 'TotalUncertainty':  [DataUnc, TotalUnc].\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from CatBoost:\n",
      " |\n",
      " |  feature_importances_\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from _CatBoostBase:\n",
      " |\n",
      " |  __copy__(self)\n",
      " |\n",
      " |  __deepcopy__(self, _)\n",
      " |\n",
      " |  __eq__(self, other)\n",
      " |      Return self==value.\n",
      " |\n",
      " |  __getstate__(self)\n",
      " |      Helper for pickle.\n",
      " |\n",
      " |  __ne__(self, other)\n",
      " |      Return self!=value.\n",
      " |\n",
      " |  __setstate__(self, state)\n",
      " |\n",
      " |  copy(self)\n",
      " |\n",
      " |  get_best_iteration(self)\n",
      " |\n",
      " |  get_best_score(self)\n",
      " |\n",
      " |  get_evals_result(self)\n",
      " |\n",
      " |  get_leaf_values(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_values : 1d-array of leaf values for all trees.\n",
      " |      Value corresponding to j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |\n",
      " |  get_leaf_weights(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      leaf_weights : 1d-array of leaf weights for all trees.\n",
      " |      Weight of j-th leaf of i-th tree is at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |\n",
      " |  get_metadata(self)\n",
      " |\n",
      " |  get_n_features_in(self)\n",
      " |\n",
      " |  get_scale_and_bias(self)\n",
      " |\n",
      " |  get_test_eval(self)\n",
      " |\n",
      " |  get_test_evals(self)\n",
      " |\n",
      " |  get_tree_leaf_counts(self)\n",
      " |      Returns\n",
      " |      -------\n",
      " |      tree_leaf_counts : 1d-array of numpy.uint32 of size tree_count_.\n",
      " |      tree_leaf_counts[i] equals to the number of leafs in i-th tree of the ensemble.\n",
      " |\n",
      " |  is_fitted(self)\n",
      " |\n",
      " |  set_feature_names(self, feature_names)\n",
      " |      Sets feature names equal to feature_names\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      feature_names: 1-d array of strings with new feature names in the same order as in pool\n",
      " |\n",
      " |  set_leaf_values(self, new_leaf_values)\n",
      " |      Sets values at tree leafs of ensemble equal to new_leaf_values.\n",
      " |\n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      new_leaf_values : 1d-array with new leaf values for all trees.\n",
      " |      It's size should be equal to sum(get_tree_leaf_counts()).\n",
      " |      Value corresponding to j-th leaf of i-th tree should be at position\n",
      " |      sum(get_tree_leaf_counts()[:i]) + j (leaf and tree indexing starts from zero).\n",
      " |\n",
      " |  set_scale_and_bias(self, scale, bias)\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from _CatBoostBase:\n",
      " |\n",
      " |  best_iteration_\n",
      " |\n",
      " |  best_score_\n",
      " |\n",
      " |  classes_\n",
      " |\n",
      " |  evals_result_\n",
      " |\n",
      " |  feature_names_\n",
      " |\n",
      " |  learning_rate_\n",
      " |\n",
      " |  n_features_in_\n",
      " |\n",
      " |  random_seed_\n",
      " |\n",
      " |  tree_count_\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from _CatBoostBase:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes inherited from _CatBoostBase:\n",
      " |\n",
      " |  __hash__ = None\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(CatBoostClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "38f2cc17-ec18-4212-a6c2-7e50adc85ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n"
     ]
    }
   ],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "\n",
    "param_grid = {\n",
    "    'iterations' : [100,200],\n",
    "    'depth': [4,6,8],\n",
    "    'learning_rate' : [0.01,0.1],\n",
    "    'l2_leaf_reg' : [1,3,5]\n",
    "}\n",
    "\n",
    "cat_model = CatBoostClassifier(verbose = 0 , auto_class_weights = 'Balanced',random_state = 42)\n",
    "grid_cat = GridSearchCV(\n",
    "    estimator = cat_model,\n",
    "    param_grid = param_grid,\n",
    "    cv = 5,\n",
    "    n_jobs = -1, verbose = 1\n",
    ")\n",
    "\n",
    "grid_cat.fit(X_train_scaled,y_train)\n",
    "best_cat = grid_cat.best_estimator_\n",
    "\n",
    "y_pred_cat = best_cat.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "44337482-e8e4-490a-ba52-6f829d7bf82d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " CatBoost Classifier Evaluation \n",
      "Best Parameters: {'depth': 8, 'iterations': 200, 'l2_leaf_reg': 1, 'learning_rate': 0.1}\n",
      "Accuracy:  0.7534\n",
      "Precision: 0.5283\n",
      "Recall:    0.6738\n",
      "F1 Score:  0.5922\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.78      0.82      1033\n",
      "           1       0.53      0.67      0.59       374\n",
      "\n",
      "    accuracy                           0.75      1407\n",
      "   macro avg       0.70      0.73      0.71      1407\n",
      "weighted avg       0.78      0.75      0.76      1407\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[808 225]\n",
      " [122 252]]\n"
     ]
    }
   ],
   "source": [
    "print(\" CatBoost Classifier Evaluation \")\n",
    "print(\"Best Parameters:\", grid_cat.best_params_)\n",
    "print(f\"Accuracy:  {accuracy_score(y_test, y_pred_cat):.4f}\")\n",
    "print(f\"Precision: {precision_score(y_test, y_pred_cat):.4f}\")\n",
    "print(f\"Recall:    {recall_score(y_test, y_pred_cat):.4f}\")\n",
    "print(f\"F1 Score:  {f1_score(y_test, y_pred_cat):.4f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred_cat))\n",
    "print(\"\\nConfusion Matrix:\\n\", confusion_matrix(y_test, y_pred_cat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "1954edd4-788b-45e5-8184-2d4453c139ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "# Store results here\n",
    "model_scores = []\n",
    "\n",
    "def evaluate_model(name, y_true, y_pred):\n",
    "    accuracy = accuracy_score(y_true, y_pred) * 100\n",
    "    precision = precision_score(y_true, y_pred)\n",
    "    recall = recall_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "    model_scores.append({\n",
    "        \"Model\": name,\n",
    "        \"Accuracy (%)\": round(accuracy, 2),\n",
    "        \"Precision\": round(precision, 4),\n",
    "        \"Recall\": round(recall, 4),\n",
    "        \"F1 Score\": round(f1, 4)\n",
    "    })\n",
    "\n",
    "def print_model_summary():\n",
    "    summary_df = pd.DataFrame(model_scores)\n",
    "    print(\"\\n Model Comparison Summary:\\n\")\n",
    "    print(summary_df.to_string(index=False))\n",
    "    return summary_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "25d1f242-63b8-4610-b0f8-367a04536a68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Model Comparison Summary:\n",
      "\n",
      "              Model  Accuracy (%)  Precision  Recall  F1 Score\n",
      "Logistic Regression         72.64     0.4909  0.7968    0.6075\n",
      "      Random Forest         72.78     0.4926  0.8048    0.6112\n",
      "            XGBoost         73.70     0.5035  0.7727    0.6097\n",
      "           CatBoost         75.34     0.5283  0.6738    0.5922\n"
     ]
    }
   ],
   "source": [
    "evaluate_model(\"Logistic Regression\", y_test, y_pred)\n",
    "evaluate_model(\"Random Forest\", y_test, y_pred_rf)\n",
    "evaluate_model(\"XGBoost\", y_test, y_pred_xgb)\n",
    "evaluate_model(\"CatBoost\", y_test, y_pred_cat)\n",
    "\n",
    "# Final summary table\n",
    "summary_df = print_model_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "386db405-df56-4403-ba03-22468183e86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_names = [\n",
    "    'gender', 'SeniorCitizen', 'Partner', 'Dependents', 'tenure',\n",
    "    'PhoneService', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
    "    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
    "    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod',\n",
    "    'MonthlyCharges', 'TotalCharges'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "57e5e927-a3d8-4aa0-a78c-d4207c62e987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def user_input_churn_predictor(model, scaler, feature_names, threshold=0.2):\n",
    "    print(\"Enter values for each of the 19 features:\")\n",
    "\n",
    "    input_values = []\n",
    "    for col in feature_names:\n",
    "        # Automatically ask for int or float\n",
    "        if col in ['tenure', 'MonthlyCharges', 'TotalCharges']:\n",
    "            val = float(input(f\"Enter {col} (e.g., 0 to 1000): \"))\n",
    "        else:\n",
    "            val = int(input(f\"Enter {col} (0 or 1 or encoded value): \"))\n",
    "        input_values.append(val)\n",
    "    \n",
    "    # Create DataFrame\n",
    "    input_df = pd.DataFrame([dict(zip(feature_names, input_values))])\n",
    "\n",
    "    # Scale the input\n",
    "    scaled_input = scaler.transform(input_df)\n",
    "\n",
    "    # Predict\n",
    "    prob = model.predict_proba(scaled_input)[0][1]\n",
    "    pred = 1 if prob > threshold else 0\n",
    "\n",
    "    # Display result\n",
    "    print(\"\\n Churn Prediction Result\")\n",
    "    print(f\"Predicted Churn: {'Yes' if pred == 1 else 'No'}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "1386a8a3-d457-4faf-8784-abc88d96705c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter values for each of the 19 features:\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter gender (0 or 1 or encoded value):  1\n",
      "Enter SeniorCitizen (0 or 1 or encoded value):  1\n",
      "Enter Partner (0 or 1 or encoded value):  0\n",
      "Enter Dependents (0 or 1 or encoded value):  1\n",
      "Enter tenure (e.g., 0 to 1000):  8\n",
      "Enter PhoneService (0 or 1 or encoded value):  0\n",
      "Enter MultipleLines (0 or 1 or encoded value):  0\n",
      "Enter InternetService (0 or 1 or encoded value):  0\n",
      "Enter OnlineSecurity (0 or 1 or encoded value):  0\n",
      "Enter OnlineBackup (0 or 1 or encoded value):  0\n",
      "Enter DeviceProtection (0 or 1 or encoded value):  0\n",
      "Enter TechSupport (0 or 1 or encoded value):  0\n",
      "Enter StreamingTV (0 or 1 or encoded value):  1\n",
      "Enter StreamingMovies (0 or 1 or encoded value):  1\n",
      "Enter Contract (0 or 1 or encoded value):  1\n",
      "Enter PaperlessBilling (0 or 1 or encoded value):  0\n",
      "Enter PaymentMethod (0 or 1 or encoded value):  0\n",
      "Enter MonthlyCharges (e.g., 0 to 1000):  99\n",
      "Enter TotalCharges (e.g., 0 to 1000):  820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Churn Prediction Result\n",
      "Predicted Churn: Yes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\danis\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:486: UserWarning: X has feature names, but StandardScaler was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_input_churn_predictor(best_rf, scaler, feature_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6782129f-3c75-4816-b2da-b996a8e68972",
   "metadata": {},
   "source": [
    "# Actionable Insights to Reduce Customer Churn\n",
    "Focus on Contract Types:\n",
    "Customers on month-to-month contracts show higher churn rates.\n",
    "âž¤ Encourage them to switch to long-term contracts by offering loyalty discounts or bonuses.\n",
    "\n",
    "Reduce High Monthly Charges:\n",
    "High charges combined with low tenure increase churn risk.\n",
    "âž¤ Offer custom discounts or bundle packages to make pricing more attractive, especially for new users.\n",
    "\n",
    "Promote Service Add-ons:\n",
    "Customers without Online Security or Tech Support features churn more.\n",
    "âž¤ Bundle these features with other plans or offer limited-time trials to increase engagement.\n",
    "\n",
    "Engage Senior Citizens More Effectively:\n",
    "Senior customers, particularly those without dependents, show slightly higher churn.\n",
    "âž¤ Simplify support channels and provide personalized communication or onboarding assistance.\n",
    "\n",
    "Improve Early Onboarding Experience:\n",
    "Churn is high among customers with tenure under 3 months.\n",
    "âž¤ Implement welcome campaigns, first-month usage guides, or satisfaction check-ins to improve retention.\n",
    "\n",
    "Increase Feature Usage Awareness:\n",
    "Low usage of features like StreamingTV or OnlineBackup correlates with churn.\n",
    "âž¤ Use personalized recommendations or in-app nudges to increase awareness and usage.\n",
    "\n",
    "Develop a Churn Monitoring Dashboard:\n",
    "âž¤ Use model output to build a live dashboard for identifying high-risk customers, their churn scores, and key influencing factors.\n",
    "âž¤ Prioritize retention outreach based on risk level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85aa73b0-66eb-427a-ac96-69296157dcad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
